"""
Generic tool environment implementation, usable with any set of tools
"""

import re
import json
import random
import traceback
from typing import Dict, List, Any, Tuple, Optional
from abc import ABC, abstractmethod
from collections import defaultdict
from copy import deepcopy

from agent.tool.tool_base import Tool

# Independent step function
def step(env: 'ToolEnv', action_text: str,student_id: str):
    """
    Execute one step of environment interaction
    
    Args:
        env: The tool environment
        action_text: Text generated by LLM
        
    Returns:
        (observation, reward, done, info)
    """
    env.steps_taken += 1
    action = env.extract_tool_call(action_text)
    print("step")
    print(action)
    if action == env.INVALID_ACTION:
        result = "Invalid tool call format. Please use <select>{\"select\": \"statement\"}</select> format."
        env._update_tracking_variables(
            response=action_text,
            action=action,
            action_is_valid=False,
            action_is_effective=False,
            reward=0
        )
        return result, env.PENALTY_FOR_INVALID, False, {"action_is_valid": False, "action_is_effective": False}
    
    tool_name = action["tool"]
    tool_args = action["args"]
    
    # Validate if the tool exists
    if tool_name not in env.tool_map:
        result = f"Unknown tool: {tool_name}"
        env._update_tracking_variables(
            response=action_text,
            action=action,
            action_is_valid=True,
            action_is_effective=False,
            reward=0
        )
        return result, env.PENALTY_FOR_INVALID, False, {"action_is_valid": True, "action_is_effective": False}
    
    # Get tool instance
    tool = env.tool_map[tool_name]
    
    # Validate tool arguments
    is_valid, error_msg = tool.validate_args(tool_args)
    if not is_valid:
        result = f"Invalid arguments for tool '{tool_name}': {error_msg}"
        env._update_tracking_variables(
            response=action_text,
            action=action,
            action_is_valid=True,
            action_is_effective=False,
            reward=0
        )
        return result, env.PENALTY_FOR_INVALID, False, {"action_is_valid": True, "action_is_effective": False}
    
    # Execute tool
    try:
        result = tool.execute(tool_args)
        reward = tool.calculate_reward(tool_args, result)
        
        # Record tool call history
        env.tool_history.append({
            "tool": tool_name,
            "args": tool_args,
            "result": result
        })
        
        # Check if max turns reached
        done = env.steps_taken >= env.max_turns
        
        env._update_tracking_variables(
            response=action_text,
            action=action,
            action_is_valid=True,
            action_is_effective=True,
            reward=reward
        )
        
        return result, reward, done, {"action_is_valid": True, "action_is_effective": True}
    except Exception as e:
        error_trace = traceback.format_exc()
        result = f"Error executing tool '{tool_name}': {str(e)}"
        
        env._update_tracking_variables(
            response=action_text,
            action=action,
            action_is_valid=True,
            action_is_effective=False,
            reward=0
        )
        
        return result, env.PENALTY_FOR_INVALID, False, {"action_is_valid": True, "action_is_effective": False}

# Batch step function
def step_batch(envs: List['ToolEnv'], action_texts: List[str], active_student_ids: List[str],num_samples:int,is_reset:bool,step:int,count:int):
    
    assert len(envs) == len(action_texts) == len(active_student_ids), \
        "Number of environments, actions and student IDs must match"
    
    tool_groups = {}       
    tool_indices = {}      
    env_indices = {}       
    student_indices = {}   
    action_map = {}        
    accs = [] # 初始化当前的准确率
    results = [None] * len(envs)
    select_final_questions = {}
    # First pass: process all actions
    for i, (env, action_text, student_id) in enumerate(zip(envs, action_texts, active_student_ids)):
        action = env.extract_tool_call(action_text)
        action_map[i] = (env, action, action_text, student_id)
        
        # Handle invalid actions immediately
        if action == env.INVALID_ACTION:
            env.steps_taken += 1
            env._update_tracking_variables(
                response=action_text,
                action=action,
                action_is_valid=False,
                action_is_effective=False,
                reward=0
            )
            results[i] = (
                "Invalid tool call format. Please use <select>{\"select\": \"statement\"}</select> format.",
                env.PENALTY_FOR_INVALID,
                False,
                {"action_is_valid": False, "action_is_effective": False, "student_id": student_id}
            )
            continue
            
        tool_name = action["tool"]
        tool_args = action["args"]
        
        # Handle unknown tools
        if tool_name not in env.tool_map:
            env.steps_taken += 1
            env._update_tracking_variables(
                response=action_text,
                action=action,
                action_is_valid=True,
                action_is_effective=False,
                reward=0
            )
            results[i] = (
                f"Unknown tool: {tool_name}",
                env.PENALTY_FOR_INVALID,
                False,
                {"action_is_valid": True, "action_is_effective": False, "student_id": student_id}
            )
            continue
            
        # Validate arguments
        tool = env.tool_map[tool_name]
        is_valid, error_msg = tool.validate_args(tool_args)
        if not is_valid:
            env.steps_taken += 1
            env._update_tracking_variables(
                response=action_text,
                action=action,
                action_is_valid=True,
                action_is_effective=False,
                reward=0
            )
            results[i] = (
                f"Invalid arguments for tool '{tool_name}': {error_msg}",
                env.PENALTY_FOR_INVALID,
                False,
                {"action_is_valid": True, "action_is_effective": False, "student_id": student_id}
            )
            continue
            
        # Group valid actions
        if tool_name not in tool_groups:
            tool_groups[tool_name] = []
            tool_indices[tool_name] = []
            env_indices[tool_name] = []
            student_indices[tool_name] = []
            
        tool_groups[tool_name].append(tool_args)
        tool_indices[tool_name].append(i)
        env_indices[tool_name].append(env)
        student_indices[tool_name].append(student_id)
    
    # Second pass: batch execution
    for tool_name, args_list in tool_groups.items():
        tool = env_indices[tool_name][0].tool_map[tool_name]
        
        # Execute with precisely matched student IDs
        batch_results, accs = tool.batch_execute(
            args_list,
            student_ids=student_indices[tool_name],
            num_samples=num_samples,
            is_reset = is_reset,
            step = step,
            count = count
        )
        select_final_questions = tool.get_tool_final_results()
        # Process results with all related indices
        for idx, env, result, args, student_id in zip(
            tool_indices[tool_name],
            env_indices[tool_name],
            batch_results,
            args_list,
            student_indices[tool_name]
        ):
            env.steps_taken += 1
            reward = tool.calculate_reward(args, result)
            done = env.steps_taken >= env.max_turns
            
            # Record tool call with student context
            env.tool_history.append({
                "tool": tool_name,
                "args": args,
                "result": result,
                "student_id": student_id
            })
            
            # Update tracking with original action data
            original_action_text = action_texts[idx]
            original_action = action_map[idx][1]
            env._update_tracking_variables(
                response=original_action_text,
                action=original_action,
                action_is_valid=True,
                action_is_effective=True,
                reward=reward
            )
            
            results[idx] = (
                result,
                reward,
                done,
                {
                    "action_is_valid": True,
                    "action_is_effective": True,
                    "student_id": student_id
                }
            )
    return results, select_final_questions, accs

class ToolEnv:
    """
    Generic tool environment class, handling tool calls, history tracking, and state
    """
    INVALID_ACTION = {"tool": "invalid", "args": {}}
    PENALTY_FOR_INVALID = 0.0
    
    def __init__(self, tools: List[Tool] = None, max_turns: int = 10):
        """
        Initialize the tool environment
        
        Args:
            tools: List of available tools
            max_turns: Maximum number of interaction turns
        """
        self.tools = tools or []
        self.tool_map = {tool.name: tool for tool in self.tools}
        self.tool_desc = [tool.get_description() for tool in self.tools]
        self.max_turns = max_turns
        self.reset_tracking_variables()

    def tools_format_func(self) -> str:
        template = """"""
        return template
        
    def reset_tracking_variables(self):
        """Reset tracking variables"""
        self.reward = 0
        self.tool_history = []  # Record tool call history
        self.steps_taken = 0
        self._actions = []  # All actions (including all LLM responses)
        self._actions_valid = []  # Correctly formatted actions
        self._actions_effective = []  # Effectively executed actions
    
    def get_tracking_variables(self) -> Dict:
        """Get statistics of tracking variables"""
        return {
            "reward": self.reward,
            "steps_taken": self.steps_taken,
            "tool_history": self.tool_history,
            "actions": self._actions,
            "actions_valid": self._actions_valid,
            "actions_effective": self._actions_effective,
        }
    
    def _update_tracking_variables(
            self, 
            response: str,
            action: Any, 
            action_is_valid: bool,
            action_is_effective: bool,
            reward: float,
        ):
        """
        Update tracking variables
        
        Args:
            response: Raw LLM response
            action: Parsed action
            action_is_valid: Whether the action format is valid
            action_is_effective: Whether the action executed successfully
            reward: Reward for the current step
        """
        self._actions.append(response)
        if action_is_valid:
            self._actions_valid.append(action)
        else:
            self._actions_valid.append(None)
        if action_is_effective:
            self._actions_effective.append(action)
        else:
            self._actions_effective.append(None)
        
        self.reward += reward if action_is_valid else (reward + self.PENALTY_FOR_INVALID)
    
    def extract_response(self, response):
        pattern = r'\bPm_\d+\b'
        match = re.search(pattern, response)
        if match:
            return match.group()
        else:
            return None
    
    def extract_tool_call(self, text: str) -> Dict:
        """
        Extract tool call from LLM output
        
        Args:
            text: Text generated by LLM
            
        Returns:
            Dictionary containing tool name and parameters
        """
        tool_call_pattern = r'<select>(.*?)</select>'
        
        tool_call_match = re.search(tool_call_pattern, text, re.DOTALL) 
        if not tool_call_match:
            print("no tool call match")
            return self.INVALID_ACTION
        
        
        tool_call_extract = self.extract_response(text) # 提取具体选择的题号
        print(f"[DEBUG] tool_call_extract: {tool_call_extract}")
        if tool_call_extract is None:
            print("tool call extract is None")
            return self.INVALID_ACTION
            
        statement = {"select": tool_call_extract}
            
        return {"tool": "select", "args": statement}
    
    def get_tool_history_context(self) -> str:
        """
        Generate tool call history context
        
        Returns:
            Formatted tool call history
        """
        if not self.tool_history:
            return "No tool call history yet."
        
        context = "Tool call history:\n"
        for i, call in enumerate(self.tool_history):
            context += f"{i+1}. Tool: {call['tool']}\n"
            context += f"   Arguments: {json.dumps(call['args'], ensure_ascii=False)}\n"
            context += f"   Result: {call['result']}\n\n"
        
        return context
    
    def get_available_tools_description(self) -> str:
        """
        Get description of available tools
        
        Returns:
            Formatted tool descriptions
        """
        if not self.tools:
            return "No tools available."
            
        descriptions = ["Available tools:"]
        for tool in self.tools:
            descriptions.append(tool.get_simple_description())
            
        return "\n\n".join(descriptions)
    
    def copy(self):
        """
        Copy the tool environment
        """
        env = ToolEnv(tools=self.tools, max_turns=self.max_turns)
        env.tool_history = deepcopy(self.tool_history)
        env.reward = self.reward
        env.steps_taken = self.steps_taken
        env._actions = deepcopy(self._actions)
        env._actions_valid = deepcopy(self._actions_valid)
        env._actions_effective = deepcopy(self._actions_effective)
        return env